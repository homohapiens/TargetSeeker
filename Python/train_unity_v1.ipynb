{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-r_cB2rqp5x"
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YSf-WhxbqtLw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Any\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecMonitor, VecEnv, SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    BaseCallback,\n",
    "    CheckpointCallback,\n",
    "    CallbackList,\n",
    "    EveryNTimesteps\n",
    ")\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "\n",
    "from supersuit import observation_lambda_v0\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "from mlagents_envs.registry import UnityEnvRegistry, default_registry\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import (\n",
    "    EngineConfig,\n",
    "    EngineConfigurationChannel,\n",
    ")\n",
    "from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment  and Engine Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values from CLI (See cli_utils.py)\n",
    "DEFAULT_ENGINE_CONFIG = EngineConfig(\n",
    "    width=84,\n",
    "    height=84,\n",
    "    quality_level=4,\n",
    "    time_scale=20,\n",
    "    target_frame_rate=-1,\n",
    "    capture_frame_rate=60,\n",
    ")\n",
    "\n",
    "# Some config subset of an actual config.yaml file for MLA.\n",
    "@dataclass\n",
    "class LimitedConfig:\n",
    "    # The local path to a Unity executable or the name of an entry in the registry.\n",
    "    env_path_or_name: str\n",
    "    base_port: int\n",
    "    base_seed: int = 0\n",
    "    num_env: int = 1\n",
    "    engine_config: EngineConfig = DEFAULT_ENGINE_CONFIG\n",
    "    visual_obs: bool = False\n",
    "    # TODO: Decide if we should just tell users to always use MultiInputPolicy so we can simplify the user workflow.\n",
    "    # WARNING: Make sure to use MultiInputPolicy if you turn this on.\n",
    "    allow_multiple_obs: bool = False\n",
    "    env_registry: UnityEnvRegistry = default_registry\n",
    "    agent_level: float = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Environment SB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unity_env_from_path_or_registry(\n",
    "    env: str, registry: UnityEnvRegistry, **kwargs: Any\n",
    ") -> UnityEnvironment:\n",
    "    env_file_exists = Path(f'{env}.exe').expanduser().absolute().exists() or \\\n",
    "                      Path(f'{env}.x86_64').expanduser().absolute().exists() or \\\n",
    "                      Path(env).expanduser().absolute().exists()\n",
    "    if env_file_exists:\n",
    "        return UnityEnvironment(file_name=env, **kwargs)\n",
    "    elif env in registry:\n",
    "        return registry.get(env).make(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Environment '{env}' wasn't a local path or registry entry\")\n",
    "        \n",
    "def make_mla_sb3_env(config: LimitedConfig, **kwargs: Any) -> VecEnv:\n",
    "    def handle_obs(obs, space):\n",
    "        if isinstance(space, gym.spaces.Tuple):\n",
    "            if len(space) == 1:\n",
    "                return obs[0]\n",
    "            # Turn the tuple into a dict (stable baselines can handle spaces.Dict but not spaces.Tuple).\n",
    "            return {str(i): v for i, v in enumerate(obs)}\n",
    "        return obs\n",
    "\n",
    "    def handle_obs_space(space):\n",
    "        if isinstance(space, gym.spaces.Tuple):\n",
    "            if len(space) == 1:\n",
    "                return space[0]\n",
    "            # Turn the tuple into a dict (stable baselines can handle spaces.Dict but not spaces.Tuple).\n",
    "            return gym.spaces.Dict({str(i): v for i, v in enumerate(space)})\n",
    "        return space\n",
    "\n",
    "    def create_env(env: str, worker_id: int) -> Callable[[], Env]:\n",
    "        def _f() -> Env:\n",
    "            engine_configuration_channel = EngineConfigurationChannel()\n",
    "            engine_configuration_channel.set_configuration(config.engine_config)\n",
    "            # Unity environment parameters \n",
    "            environment_configuration_channel = EnvironmentParametersChannel()\n",
    "            environment_configuration_channel.set_float_parameter('agent_level', config.agent_level)\n",
    "            kwargs[\"side_channels\"] = kwargs.get(\"side_channels\", []) + [\n",
    "                engine_configuration_channel,\n",
    "                environment_configuration_channel\n",
    "            ]\n",
    "            unity_env = _unity_env_from_path_or_registry(\n",
    "                env=env,\n",
    "                registry=config.env_registry,\n",
    "                worker_id=worker_id,\n",
    "                base_port=config.base_port,\n",
    "                seed=config.base_seed + worker_id,\n",
    "                **kwargs,\n",
    "            )\n",
    "            new_env = UnityToGymWrapper(\n",
    "                unity_env=unity_env,\n",
    "                uint8_visual=config.visual_obs,\n",
    "                allow_multiple_obs=config.allow_multiple_obs,\n",
    "            )\n",
    "            new_env = observation_lambda_v0(new_env, handle_obs, handle_obs_space)\n",
    "            return new_env\n",
    "\n",
    "        return _f\n",
    "\n",
    "    env_facts = [\n",
    "        create_env(config.env_path_or_name, worker_id=x) for x in range(config.num_env)\n",
    "    ]\n",
    "    return SubprocVecEnv(env_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log scalar value (here a random variable)\n",
    "        value = np.random.random()\n",
    "        self.logger.record('z_value', value)\n",
    "        return True\n",
    "    \n",
    "    def _on_training_end(self) -> None:\n",
    "        print('...Finished Training!')\n",
    "        os.rename(f'results/{run_id}/TB_Logs_1', f'results/{run_id}/TB_Logs')\n",
    "\n",
    "class SummaryCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Summary callback for printing values on console.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0, n_episodes=100, log_dir=None):\n",
    "        super(SummaryCallback, self).__init__(verbose)\n",
    "        self.n_episodes = n_episodes\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        print('Started Training...')\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log scalar value (here a random variable)\n",
    "        x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "        mean_reward = np.mean(y[-self.n_episodes:])\n",
    "        std_reward = np.std(y[-self.n_episodes:])\n",
    "        \n",
    "        print(f'Step {self.num_timesteps}, \\\n",
    "        Time Elapsed {time.time()-self.start_time:.3f}s, \\\n",
    "        Mean Reward {mean_reward:.3f}, \\\n",
    "        Std Reward {std_reward:.3f}')\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(initial_value: float, schedule: str) -> Callable[[float], float]:\n",
    "    \n",
    "    if schedule == 'linear':\n",
    "        def func(progress_remaining: float) -> float:\n",
    "            return progress_remaining * initial_value\n",
    "    elif schedule == 'constant':\n",
    "        def func(progress_remaining: float) -> float:\n",
    "            return initial_value\n",
    "    else:\n",
    "        raise NameError(f'Invalid schedule: {schedule}!')\n",
    "    return func\n",
    "\n",
    "\n",
    "def check_valid_run_id(run_id, force=False):\n",
    "    dir_path = f'results/{run_id}'\n",
    "    if os.path.isdir(dir_path):\n",
    "        if force:\n",
    "            shutil.rmtree(dir_path)\n",
    "        else:\n",
    "            raise\n",
    "    os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "NUM_ENVS = 8\n",
    "build_path = 'C:/main/MLAgents/TargetSeeker/Build/win/TargetSeeker'\n",
    "run_id = 'test_run_0'\n",
    "force = True\n",
    "summary_freq = 10_000\n",
    "check_point_freq = 1_000_000\n",
    "n_check_points = 5\n",
    "agent_level = 0\n",
    "\n",
    "config=LimitedConfig(\n",
    "        env_path_or_name=build_path,  # Can use any name from a registry or a path to your own unity build.\n",
    "        base_port=5005,\n",
    "        base_seed=0,\n",
    "        num_env=NUM_ENVS,\n",
    "        allow_multiple_obs=True,\n",
    "        agent_level=agent_level\n",
    "    )\n",
    "\n",
    "env = make_mla_sb3_env(\n",
    "    config=config,\n",
    "    no_graphics=True,  # Set to false if you are running locally and want to watch the environments move around as they train.\n",
    ")\n",
    "\n",
    "# Validate run-id\n",
    "check_valid_run_id(run_id, force)\n",
    "\n",
    "# Helps gather stats for our eval() calls later so we can see reward stats.\n",
    "env = VecMonitor(env, filename=f'results/{run_id}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 250K should train to a reward ~= 0.90 for the \"Basic\" environment.\n",
    "# We set the value lower here to demonstrate just a small amount of trianing.\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10240\n",
    "UPDATES = 2000\n",
    "TOTAL_TAINING_STEPS_GOAL = BUFFER_SIZE * UPDATES\n",
    "BETA = 0.0005\n",
    "N_EPOCHS = 3\n",
    "STEPS_PER_UPDATE = BUFFER_SIZE / NUM_ENVS\n",
    "\n",
    "#Policy and Value function with 2 layers of 128 units each and no shared layers.\n",
    "policy_kwargs = {\"net_arch\" : [{\"pi\": [128,128], \"vf\": [128,128]}]}\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    learning_rate=scheduler(0.0003, 'linear'),\n",
    "    clip_range=scheduler(0.2, 'linear'),\n",
    "    clip_range_vf=scheduler(0.2, 'linear'),\n",
    "    tensorboard_log=f'results/{run_id}',\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    n_steps=int(STEPS_PER_UPDATE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    ent_coef=BETA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training...\n",
      "Step 10000,         Time Elapsed 7.297s,         Mean Reward -0.984,         Std Reward 0.387\n",
      "Step 20000,         Time Elapsed 14.405s,         Mean Reward -1.041,         Std Reward 0.212\n",
      "Step 30000,         Time Elapsed 21.565s,         Mean Reward -1.004,         Std Reward 0.319\n",
      "Step 40000,         Time Elapsed 28.926s,         Mean Reward -1.007,         Std Reward 0.343\n",
      "Step 50000,         Time Elapsed 36.154s,         Mean Reward -0.943,         Std Reward 0.507\n",
      "Step 60000,         Time Elapsed 43.395s,         Mean Reward -0.969,         Std Reward 0.480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_callback = TensorboardCallback()\n",
    "checkpoint_callback = CheckpointCallback(save_freq=max(int(check_point_freq/NUM_ENVS),1), save_path=f'results/{run_id}/Checkpoints/',\n",
    "                                         name_prefix='TargetSeeker')\n",
    "summary_callback = EveryNTimesteps(n_steps=summary_freq, callback=SummaryCallback(n_episodes=100, log_dir=f'results/{run_id}'))\n",
    "\n",
    "# Chain all callbacks\n",
    "callback = CallbackList([tb_callback, checkpoint_callback, summary_callback])\n",
    "\n",
    "# Start train\n",
    "try:\n",
    "    model.learn(total_timesteps=TOTAL_TAINING_STEPS_GOAL, reset_num_timesteps=True, callback=callback, tb_log_name='TB_Logs')\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    model.save(f'results/{run_id}/TargetSeeker')\n",
    "    del model\n",
    "    print(\"Saved model\")\n",
    "    env.close()\n",
    "    print(\"Closed environment\")\n",
    "    \n",
    "# for i in range(UPDATES):\n",
    "    # print(f'\\rTraining round {i + 1}/{UPDATES}', end='')\n",
    "    # model.learn(total_timesteps=BUFFER_SIZE, reset_num_timesteps=(i == 0), callback=callback, tb_log_name='TB_Logs')\n",
    "    # model.policy.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1lIx3_l24OP"
   },
   "source": [
    "### Close the environment\n",
    "Frees up the ports being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdWG6_SqtNtv",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    print(\"Closed environment\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Colab-UnityEnvironment-1-Run.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (mla-gym)",
   "language": "python",
   "name": "mla-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
